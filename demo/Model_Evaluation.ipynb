{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Evaluate Trained Model:\n",
    "\n",
    "## Introduction:\n",
    "This notebook goes over the evaluation process. It loads in the trained models ($5$ by default) and applies them to the testing images (images not yet seen by the models when training). In this project, the testing dataset consists of images from the start of $2007$ to the end of $2019$. While more data is available post $2019$, they were purposefully excluded due to unexpected trends resulting from the COVID pandemic.\n",
    "\n",
    "Furthermore, this notebook only goes over the $I5$ base images (training images consists of $5$ day periods) since the computer used to train the models could only compile the $I5$ architectures ($I20$ and $I60$ architectures are available in this project, yet may require substantial computing resources due to their more complex structures). \n",
    "\n",
    "The evaluation process consists of loading in the models, applying the models to the images to get predicted probabilities, sorting the returns based on predicted probabilities, and sorting model's estimates into decile portfolios to view overall model's performance. \n",
    "\n",
    "**NOTE**: This notebook documents the results compiled previously from my personal computer. Please view the `Data_Acquisition.ipynb` and `Model_Training.ipynb` to generate all necessary files if you want to compile this notebook with your own personal dataset. Furthermore, please make sure you have the $I5$ OHLC base images and their respective models in the `data` directory to run this notebook. You can download these datasets from the README file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Packages:\n",
    "In order to correctly evaluate the model, we'll use the following packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all necessary packages.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "import src.training.evaluate as stock_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Working Directories:\n",
    "Please use the following code to establish the base directory and the working directories containing all the dataset. \n",
    "\n",
    "**Note:** The directories below assumes the user used the notebooks/scripts to create the following datasets and models. If you have saved the dataset or models to other directories, please revise the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the base directory to be in .\\stock_code.\n",
    "base_dir = os.path.abspath(os.path.join('.', '..'))\n",
    "\n",
    "## Uncomment the code below to view your current working directory to check if it's correct.\n",
    "# print(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Establish the directories that are needed to compile this notebook.\n",
    "\n",
    "## The directory that stores the testing images.\n",
    "test_dir = os.path.join(base_dir, 'data', 'processed_img', 'stock_img', 'I5R5', 'base_img', 'testing_dataset')\n",
    "\n",
    "## The directory that stores the true weekly returns for each testing image.\n",
    "ret_dir = os.path.join(base_dir, 'data', 'processed_img', 'stock_img', 'I5R5', 'returns')\n",
    "\n",
    "## The directory that stores the trained model's weights.\n",
    "models_dir = os.path.join(base_dir, 'data', 'saved_models', 'I5R5', 'base_img')\n",
    "\n",
    "## The directory that stores the predicted probabilities for each testing image.\n",
    "pred_dir = os.path.join(base_dir, 'data', 'predictions', 'I5R5', 'base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Dictionary:\n",
    "In order to make sure to keep track of the images belonging to each firm, we can create a dictionary. The `keys` will be the firm's name and the `value` will be the OHLC images for the firm from 2007 to 2019. Given the path to the testing images, a `defaultdict` will be created where each firm and its respective images will be appended to the dictionary. The images will be loaded in as an array using the function `tf.keras.preprocessing.image.load_img` and `tf.keras.preprocessing.image.img_to_array`. Finally, the images will be sorted based on their week number when assigned into the dictionary. To see more in depth, please check the `evaluate.py` module in `src.training`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Dictionary Created!\n"
     ]
    }
   ],
   "source": [
    "## Creates an image dictionary storing all the images for each firm.\n",
    "img_dict = stock_eval.create_img_dict(test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Model to Images:\n",
    "Now that the image dictionary has been created, we can apply the models to each of the firm's images and get the probability of the stock going up or down in the future. Given the path to the saved models, we can use the `get_avg_prediction` in `sort_eval` to perform an ensemble voting prediction. Each model will be used to obtain the probability of a stock going up or down; the probabilities for each firm will be stored as an `np.array`. These predicted probabilities made by each model will be appended to a list, which we'll then use `np.mean` to find the average predicted values for each image. The output value would be a `np.array` of shape (number of weeks, number of firms), where each cell will be the predicited probability that the stock price will go up for a certain firm at a certain week. These values will be the output we'll be using for later analysis. This process can be done using the `get_avg_prediction` function in the `evaluate.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction Finished!\n"
     ]
    }
   ],
   "source": [
    "## Apply the models to images and get average predicted probabilities.\n",
    "\n",
    "## The average predicted probabilities is stored as an np.array, where the first\n",
    "## index stores the probabilities for the first firm and the second index stores the\n",
    "## probabilities for the second firm, and so forth.\n",
    "pred_avg = stock_eval.get_avg_prediction(img_dict, models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5173253 ]\n",
      " [0.5598717 ]\n",
      " [0.58961904]\n",
      " [0.549301  ]\n",
      " [0.46375316]]\n"
     ]
    }
   ],
   "source": [
    "## View the first five weeks predicted probabilities of each image for the first firm.\n",
    "print(pred_avg[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save the predicted probabilities to a directory storing the predictions. By using the function `save_predictions`, the predicted values will be saved as a `.csv` file for each firm in the image dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Predictions Saved Successfully!\n"
     ]
    }
   ],
   "source": [
    "## Save all the respective predictions of each firm to designated directory.\n",
    "stock_eval.save_predictions(img_dict, pred_avg, pred_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results:\n",
    "Now that we've obtained our predicted values for each firm, we can analyze and evaluate the results. First, we'll load in the actual returns from our return directory, and their respective predicted probabilities from our directory storing the model's output. We'll now have two arrays of shape (number of weeks, number of firms, 1), where one array contains the historical returns and the other array contains the output from the model. After loading in these separate datasets, we'll use `np.hstack` on each array to horizontally stack so the the each row represent a specific week while each column shows the info for a specific firm (probability from the model or historical return). Our resulting shape for each return and predicted array will be (number of weeks, number of firms). This process is shown below using the function `returns_based_predictions`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get arrays for historical returns and model's predictions.\n",
    "ret_stack, pred_stack = stock_eval.returns_based_predictions(ret_dir, pred_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can sort the historical returns based on their predicted probabilities. First, we'll sort the predicted probabilities in each row of `pred_stack` in ascending order. We'll use `np.argsort` to keep track of the sorted indices. We'll then use the sorted indices to sort the return values in `ret_stack`, so that the historical returns will be sorted based on their probabilities predicted by the model. This will be done for each row (the week index) in `ret_stack`. So, the historical return for index 0 of row 0 in the sorted array will be the actual weekly return for the lowest predicted probability of the model in the first week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sort the returns based on their predicted probabilities.\n",
    "ret_sorted = stock_eval.sort_returns(ret_stack, pred_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the average annual returns of our sorted firms, we'll use the Compound Annual Growth Rate formula. \n",
    "\\begin{equation*}\n",
    "CAGR = \\left( \\frac{Value_{final}}{Value_{begin}} \\right)^{\\frac{1}{t}} - 1 = \\prod_{i = 1}^{\\text{num of weeks}} \\left( \\text{Weekly Returns}_{i} + 1 \\right)^{\\frac{1}{\\text{Num of Years}}} - 1\n",
    "\\end{equation*}\n",
    "Our returns are listed in our `ret_sorted` array, where each row marks the historical returns of a certain week sorted in their respective predicted probabilities. Hence, we can add $1$ to all the values in the array and multiply all the rows to get the overall growth across the span of $7$ years. In order to get the compound annual growth rate, we need to raise these values to the power of $\\frac{1}{7}$ and subtract $1$. This process is done using the `get_comp_returns` function, shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Returns\n",
      "0    -0.757282\n",
      "1    -0.792960\n",
      "2    -0.662229\n",
      "3    -0.407155\n",
      "4    -0.746025\n",
      "...        ...\n",
      "1087  0.656889\n",
      "1088  0.613227\n",
      "1089  0.977258\n",
      "1090  0.270810\n",
      "1091  0.698200\n",
      "\n",
      "[1092 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "## Get the annual compound returns and save it to a Dataframe.\n",
    "ret_df = stock_eval.get_comp_return(ret_sorted)\n",
    "\n",
    "## Show the annual growth rates for each predicted probability.\n",
    "print(ret_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now track the annual compound growth rate for our predicted probabilities. The first element in `ret_df` shows the annual growth for our lowest predicted probabilites while the last element shows the annual growth for our highest predicted probabilities. We'll now sort these values into a decile portfolio by first equally separating the values into $10$ groups using the function `pd.cut`. For each bin, we'll calculate the average of the values to get the average annual growth for that particular decile group. Hence, we can now clearly see the performance of the model by viewing the average annual growth rate per decile, with the first decile being the lowest predicted probability while the last decile being the highest predicted probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decile 1 mean: -0.2720944385058328\n",
      "Decile 2 mean: 0.00984343890829152\n",
      "Decile 3 mean: 0.021044340640320944\n",
      "Decile 4 mean: 0.06459797501896818\n",
      "Decile 5 mean: 0.10604151658975122\n",
      "Decile 6 mean: 0.14210747969697113\n",
      "Decile 7 mean: 0.182349429766024\n",
      "Decile 8 mean: 0.24672336338500783\n",
      "Decile 9 mean: 0.32090490415409295\n",
      "Decile 10 mean: 0.42171993331102015\n"
     ]
    }
   ],
   "source": [
    "## Separates the return values equally into 10 deciles. \n",
    "ret_df['Decile'] = pd.cut(ret_df.index, bins = 10, labels = False)\n",
    "\n",
    "## Find the average annual growth rate per decile.\n",
    "decile_means = ret_df.groupby('Decile')['Returns'].mean()\n",
    "\n",
    "## Print out the average growth rate per decile.\n",
    "for i, mean in enumerate(decile_means):\n",
    "    print(f'Decile {i + 1} mean: {mean}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stockenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
